이 글은 'HTTP 완벽가이드'라는 책을 읽으면서 제가 이해한 내용을 바탕으로 정리한 글입니다. 
혹시 이 글을 보게 되신다면 비판적인 시각으로 보시는 것을 추천 드립니다.

<img src="http://image.yes24.com/goods/15381085/XL" alt="HTTP 완벽 가이드" style="zoom:67%;" />

---

## 웹 로봇

#### 1. 크롤러와 크롤링

- 웹 크롤러는 웹페이지를 가져오고, 그 웹페이지가 가리키는 웹페이지를 또 가져오는 웹 순회 로봇
- 인터넷 검색엔진은 웹을 돌아다니면서 그들이 만나는 모든 문서를 끌어오기 위해 크롤러 사용



1. **'루트 집합'**
   - 크롤러가 방문을 시작하는 URL들의 초기 집합은 루트 집합이라 불림
2. **링크 추출과 상대 링크 정상화**
3. **순환 피하기**
   - 웹 순한을 막기 위해 조심해야함
4. **루프와 중복**
   1. 웹 순환은 최소한 다음의 세가지 이유로 인해 크롤러에게 해롭다
      - 순환은 크롤러가 네트워크 대역폭을 다 차지하고 그 어떤 페이지도 가져올 수 없게 되버림
      - 크롤러가 같은 페이지를 반복해서 가져오면 고스란히 웹 서버의 부담이 됨
      - 애플리케이션은 자신을 쓸모없게 만드는 중복된 콘텐츠로 넘쳐나게 될 것
5. **빵 부스러기의 흔적**
   1. 대규모 웹 크롤러에게 유용한 기법
      - **트리와 해시 테이블**
        1. 복잡한 로봇이라면 방문한 URL을 추적하기 위해 검색 트리나 해시 테이블을 사용
        2. URL을 훨씬 빨리 찾을 수 있게 해줌
      - **느슨한 존재 비트맵**
      - **체크포인트**
      - **파티셔닝**
6. **별칭과 로봇 순환**
7. **URL 정규화 하기**
   - 대부분의 웹 로봇은 표준 형식으로 '정규화' 함스로써 다른 URL과 같은 리소스를 가리키고 있음이 확실한 것들을 미리 제거하려 시도
     1. 포트 번호가 명시되지 않았다면, 호스트 명에 '.80' CNRK
     2. 모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환
     3. '#' 태그 제거
8. **루프와 중복 피하기**
   - **URL 정규화**
   - **너비 우선 크롤링**
   - **스로틀링**
     - 로봇이 가져올 수 있는 페이지 숫자 제한
   - **URL 크기 제한**
   - **URL/사이트 블랙리스트**
   - **패턴발견**
   - **콘텐츠 지문**
     - 체크섬을 활용하여 동적 웹사이트의 중복을 감지
   - **사람의 모니터링**

